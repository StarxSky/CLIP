{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:39:49.551075Z","iopub.execute_input":"2022-09-09T11:39:49.554188Z","iopub.status.idle":"2022-09-09T11:39:50.796441Z","shell.execute_reply.started":"2022-09-09T11:39:49.554143Z","shell.execute_reply":"2022-09-09T11:39:50.794997Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/StarxSky/CLIP.git\n!pip install --upgrade torch==1.12.1\n!pip install --upgrade torchvision","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:39:50.798889Z","iopub.execute_input":"2022-09-09T11:39:50.799679Z","iopub.status.idle":"2022-09-09T11:41:49.993130Z","shell.execute_reply.started":"2022-09-09T11:39:50.799627Z","shell.execute_reply":"2022-09-09T11:41:49.991970Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<div style='color: #216969;\n           background-color: #EAF6F6;\n           font-size: 200%;\n           border-radius:15px;\n           text-align:center;\n           font-weight:600;\n           border-style: solid;\n           border-color: dark green;\n           font-family: \"Verdana\";'>\nNotebook Imports\n","metadata":{}},{"cell_type":"code","source":"import os \nimport clip\nimport torch \nimport skimage\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom torch.backends import mps\nfrom torchvision.datasets import CIFAR100\n\n# device\nif mps.is_available() :\n    device = torch.device('mps')\n\nelif torch.cuda.is_available() :\n    device = torch.device('cuda')\n\nelse :\n    device = torch.device('cpu')\n\nprint(f'CLIP Version(PyTorch) :{clip.version}')\nprint(f'Device :{device}')\n\n# Load models\nmodel, preprocess = clip.load(name='ViT-B/32', device=device, download_root='./Pre_Models/')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:58:01.680659Z","iopub.execute_input":"2022-09-09T11:58:01.681079Z","iopub.status.idle":"2022-09-09T11:58:05.965160Z","shell.execute_reply.started":"2022-09-09T11:58:01.681043Z","shell.execute_reply":"2022-09-09T11:58:05.963929Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"<div style='color: #216969;\n           background-color: #EAF6F6;\n           font-size: 200%;\n           border-radius:15px;\n           text-align:center;\n           font-weight:600;\n           border-style: solid;\n           border-color: dark green;\n           font-family: \"Verdana\";'>\nImporting Data\n","metadata":{}},{"cell_type":"code","source":"# images in skimage to use and their textual descriptions\ndescriptions = {\n    \"page\": \"a page of text about segmentation\",\n    \"chelsea\": \"a facial photo of a tabby cat\",\n    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n    \"rocket\": \"a rocket standing on a launchpad\",\n    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n    \"camera\": \"a person looking at a camera on a tripod\",\n    \"horse\": \"a black-and-white silhouette of a horse\", \n    \"coffee\": \"a cup of coffee on a saucer\"\n}","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:43:33.820442Z","iopub.execute_input":"2022-09-09T11:43:33.821291Z","iopub.status.idle":"2022-09-09T11:43:33.827146Z","shell.execute_reply.started":"2022-09-09T11:43:33.821249Z","shell.execute_reply":"2022-09-09T11:43:33.826057Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"original_images = []\nimages = []\ntexts = []\nplt.figure(figsize=(16, 5))\n\nfor filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n    name = os.path.splitext(filename)[0]\n    if name not in descriptions:\n        continue\n\n    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n  \n    plt.subplot(2, 4, len(images) + 1)\n    plt.imshow(image)\n    plt.title(f\"{filename}\\n{descriptions[name]}\")\n    plt.xticks([])\n    plt.yticks([])\n\n    original_images.append(image)\n    images.append(preprocess(image))\n    texts.append(descriptions[name])\n\nplt.tight_layout()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:54:53.284373Z","iopub.execute_input":"2022-09-09T11:54:53.284741Z","iopub.status.idle":"2022-09-09T11:54:54.236799Z","shell.execute_reply.started":"2022-09-09T11:54:53.284711Z","shell.execute_reply":"2022-09-09T11:54:54.235807Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"len(images)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:54:57.288833Z","iopub.execute_input":"2022-09-09T11:54:57.289372Z","iopub.status.idle":"2022-09-09T11:54:57.296779Z","shell.execute_reply.started":"2022-09-09T11:54:57.289338Z","shell.execute_reply":"2022-09-09T11:54:57.295542Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"image_input = torch.tensor(np.stack(images)).to(device)\ntext_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\nimage_input.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:54:59.748716Z","iopub.execute_input":"2022-09-09T11:54:59.749101Z","iopub.status.idle":"2022-09-09T11:54:59.763859Z","shell.execute_reply.started":"2022-09-09T11:54:59.749067Z","shell.execute_reply":"2022-09-09T11:54:59.762951Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"<div style='color: #216969;\n           background-color: #EAF6F6;\n           font-size: 200%;\n           border-radius:15px;\n           text-align:center;\n           font-weight:600;\n           border-style: solid;\n           border-color: dark green;\n           font-family: \"Verdana\";'>\nGive Model with data\n","metadata":{}},{"cell_type":"code","source":"# 将数据喂入模型CLIP\nwith torch.no_grad():\n    image_features = model.encode_image(image_input).float()\n    text_features = model.encode_text(text_tokens).float()\n    \nprint(f'Image Features Shape :{image_features.shape}')\nprint(f'Text Features Shape :{text_features.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:55:01.970114Z","iopub.execute_input":"2022-09-09T11:55:01.970484Z","iopub.status.idle":"2022-09-09T11:55:02.005840Z","shell.execute_reply.started":"2022-09-09T11:55:01.970453Z","shell.execute_reply":"2022-09-09T11:55:02.004834Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# 计算余弦相似度\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\nsimilarity = image_features.cpu().numpy() @ text_features.cpu().numpy().T\n\n\nprint(f'logits_per_image shape:  {similarity.shape}') # 图像\nprint(f'logits_per_text shape:  {similarity.T.shape}') # 文本","metadata":{"execution":{"iopub.status.busy":"2022-09-09T11:56:17.690793Z","iopub.execute_input":"2022-09-09T11:56:17.691826Z","iopub.status.idle":"2022-09-09T11:56:17.700989Z","shell.execute_reply.started":"2022-09-09T11:56:17.691777Z","shell.execute_reply":"2022-09-09T11:56:17.699848Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"* The above code is equivalent to the following code(上面的代码等效于下面的代码)\n\n```python\nwith torch.no_grad():\n    Image_P, logits_per_text = model(image_input, text_inputs)\nprint(Image_P.shape)\n```\n#### In this\n* `similarity` = `Image_P`\n* `similarity。T`  = `logits_per_text`\n#### Model Forward Code:\n```python\ndef forward(self, image, text):\n        image_features = self.encode_image(image)\n        text_features = self.encode_text(text)\n\n        # normalized features\n        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=1, keepdim=True)# <<<<====这里的shape[batch, text_nums]\n\n        # .......\n        \n        similarity = logit_scale * image_features @ text_features.t()# <<<<====将图像特征和文本特征(已经转置)相乘[Multiplying image features and text features (already transposed)]\n        \n        logits_per_text = similarity.t() # <<<<====这里将图像的预测结果进行转置得出文本的预测信息[Here the prediction result of the image is transposed to derive the prediction information of the text]\n        \n        return similarity, logits_per_text\n    \n ```","metadata":{}},{"cell_type":"markdown","source":"<div style='color: #216969;\n           background-color: #EAF6F6;\n           font-size: 200%;\n           border-radius:15px;\n           text-align:center;\n           font-weight:600;\n           border-style: solid;\n           border-color: dark green;\n           font-family: \"Verdana\";'>\nThe Easy Way","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    Image_P, logits_per_text = model(image_input, text_tokens)\n    Image_P = Image_P.softmax(dim=-1)# -1 is Adaptive\nprint(Image_P.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T12:13:10.315406Z","iopub.execute_input":"2022-09-09T12:13:10.315778Z","iopub.status.idle":"2022-09-09T12:13:10.352816Z","shell.execute_reply.started":"2022-09-09T12:13:10.315745Z","shell.execute_reply":"2022-09-09T12:13:10.351727Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"<div style='color: #216969;\n           background-color: #EAF6F6;\n           font-size: 200%;\n           border-radius:15px;\n           text-align:center;\n           font-weight:600;\n           border-style: solid;\n           border-color: dark green;\n           font-family: \"Verdana\";'>\nVisualizing the images\n","metadata":{}},{"cell_type":"code","source":"count = len(descriptions)\n\nplt.figure(figsize=(20, 14))\nplt.imshow(similarity, vmin=0.1, vmax=0.3)\nplt.colorbar()\nplt.yticks(range(count), texts, fontsize=18)\nplt.xticks([])\nfor i, image in enumerate(original_images):\n    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\nfor x in range(similarity.shape[1]):\n    for y in range(similarity.shape[0]):\n        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n\nfor side in [\"left\", \"top\", \"right\", \"bottom\"]:\n  plt.gca().spines[side].set_visible(False)\n\nplt.xlim([-0.5, count - 0.5])\nplt.ylim([count + 0.5, -2])\n\nplt.title(\"Cosine similarity between text and image features\", size=20)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T12:08:35.716087Z","iopub.execute_input":"2022-09-09T12:08:35.716666Z","iopub.status.idle":"2022-09-09T12:08:36.716951Z","shell.execute_reply.started":"2022-09-09T12:08:35.716620Z","shell.execute_reply":"2022-09-09T12:08:36.715855Z"},"trusted":true},"execution_count":47,"outputs":[]}]}
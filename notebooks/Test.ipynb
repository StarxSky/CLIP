{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Device INFO==========\n",
      "PyTorch Version :1.12.1+cpu\n",
      "Device :cpu\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda\\envs\\ML\\lib\\site-packages\\clip\\__init__.py:27: UserWarning: This PyTorch version 1.12.1 or higher is Support Metal GPU Boost!\n",
      "  warnings.warn(\"This PyTorch version 1.12.1 or higher is Support Metal GPU Boost!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Version(PyTorch) :1.12.1+cpu\n",
      "Device :cpu\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import clip\n",
    "import torch \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch.backends import mps\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# device\n",
    "if mps.is_available() :\n",
    "    device = torch.device('mps')\n",
    "\n",
    "elif torch.cuda.is_available() :\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "else :\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'CLIP Version(PyTorch) :{clip.version}')\n",
    "print(f'Device :{device}')\n",
    "\n",
    "# Load models\n",
    "model, preprocess = clip.load(name='ViT-B/32', device=device, download_root='./Pre_Models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x0000020B8D419160>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Type :<class 'PIL.Image.Image'>\n",
      "Image_inputs Shape :torch.Size([1, 3, 224, 224])\n",
      "Text_inputs Shape :torch.Size([100, 77])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the inputs\n",
    "image, class_id = cifar100[0]\n",
    "print(f'Image Type :{type(image)}')\n",
    "\n",
    "\n",
    "\n",
    "image_input = preprocess(image).unsqueeze(0).to(device) # processed image\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device) # processed texts\n",
    "\n",
    "\n",
    "print(f'Image_inputs Shape :{image_input.shape}')\n",
    "print(f'Text_inputs Shape :{text_inputs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "We use a case-insensitive tokenizer, which can be invoked using `clip.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,  3306,  1002,   256, 49407,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.tokenize('Hello world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the output of putting image & text data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Features Shape :torch.Size([1, 512])\n",
      "Text Features Shape :torch.Size([100, 512])\n"
     ]
    }
   ],
   "source": [
    "# Calculate features\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "print(f'Image Features Shape :{image_features.shape}')\n",
    "print(f'Text Features Shape :{text_features.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape:  torch.Size([1, 100])\n",
      "logits_per_text shape:  torch.Size([100, 1])\n",
      "预测的图像的信息的大小 :(1, 100)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    Image_P, logits_per_text = model(image_input, text_inputs)\n",
    "\n",
    "    P_Image = Image_P.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(f'预测的图像的信息的大小 :{P_Image.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape:  torch.Size([1, 100])\n",
      "logits_per_text shape:  torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "# 将图像-文本送入网络\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# 计算余弦相似度\n",
    "image_features /= image_features.norm(dim=1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=1, keepdim=True)\n",
    "similarity = image_features @ text_features.T\n",
    "\n",
    "\n",
    "print(f'logits_per_image shape:  {similarity.shape}') # 图像\n",
    "print(f'logits_per_text shape:  {similarity.T.shape}') # 文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上方的计算实际上是CLIP模型的forward的计算过程（等同于下方的代码）\n",
    "* `similarity` = `Image_P`\n",
    "* `similarity` 的转置 = `logits_per_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape:  torch.Size([1, 100])\n",
      "logits_per_text shape:  torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    Image_P, logits_per_text = model(image_input, text_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的计算过程\n",
    "```python\n",
    "def forward(self, image, text):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)# <<<<====这里的shape[batch, text_nums]\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()# <<<<====将图像特征和文本特征(已经转置)相乘\n",
    "        #\n",
    "        print(f'logits_per_image shape:  {logits_per_image.shape}')\n",
    "        logits_per_text = logits_per_image.t() # <<<<====这里将图像的预测结果进行转置得出文本的预测信息\n",
    "        #\n",
    "        print(f'logits_per_text shape:  {logits_per_text.shape}') \n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_image, logits_per_text\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52b219f090789875c5d110bd153026c117ac01241b392d55f0c32bcdd920e7c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
